# -*- coding: utf-8 -*-
"""HousingPricesML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SvYx4Ohrvs2C0u4MIhvNrwChqVW4L9Cf

# **Importing Data and libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data=pd.read_csv("housing.csv")
data

#data.info()
data.dropna(inplace=True)
data.info()

"""# **Establishing dependent and independent variables**"""

from sklearn.model_selection import train_test_split
x=data.drop(['median_house_value'], axis=1)#axis=1=>we are dropping a column
y=data['median_house_value']

"""# **Splitting in train/test**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

train_data=x_train.join(y_train)
train_data

"""# **Data Exploration-histogram, corr map, heat map**"""

train_data.hist()

#train_data.corr()
plt.figure(figsize=(6,6))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

"""# **Data pre-processing**"""

train_data['total_rooms']=np.log(train_data['total_rooms']+1)
train_data['total_bedrooms']=np.log(train_data['total_bedrooms']+1)
train_data['population']=np.log(train_data['population']+1)
train_data['households']=np.log(train_data['households']+1)
train_data.hist()

train_data.ocean_proximity.value_counts()
#each categorical value from ocean proximity column turns into an individual feature(column), with binary values
train_data = train_data.join(pd.get_dummies(train_data.ocean_proximity)).drop(['ocean_proximity'], axis=1)#dropping a column
train_data

plt.figure(figsize=(14,9))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

plt.figure(figsize=(15,8))
sns.scatterplot(x='latitude', y='longitude', data=train_data, hue='median_house_value', palette='coolwarm')

train_data['bedroom_ration']=train_data['total_bedrooms']/train_data['total_rooms']
train_data['household_rooms']=train_data['total_rooms']/train_data['households']

plt.figure(figsize=(14,9))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

"""# **Linear Regression**"""

from sklearn.linear_model import LinearRegression
x_train, y_train=train_data.drop(['median_house_value'], axis=1), train_data['median_house_value']
reg=LinearRegression()
reg.fit(x_train, y_train)

test_data=x_test.join(y_test)

test_data['total_rooms']=np.log(test_data['total_rooms']+1)
test_data['total_bedrooms']=np.log(test_data['total_bedrooms']+1)
test_data['population']=np.log(test_data['population']+1)
test_data['households']=np.log(test_data['households']+1)
test_data.columns

test_data=x_test.join(y_test)
test_data.columns
test_data['total_rooms']=np.log(test_data['total_rooms']+1)
test_data['total_bedrooms']=np.log(test_data['total_bedrooms']+1)
test_data['population']=np.log(test_data['population']+1)
test_data['households']=np.log(test_data['households']+1)

#test_data = test_data.join(pd.get_dummies(test_data.ocean_proximity)).drop(['ocean_proximity'], axis=1)#dropping a column

test_data['bedroom_ration']=test_data['total_bedrooms']/test_data['total_rooms']
test_data['household_rooms']=test_data['total_rooms']/test_data['households']

x_test, y_test=test_data.drop(['median_house_value'], axis=1), test_data['median_house_value']

x_test, y_test=test_data.drop(['median_house_value'], axis=1), test_data['median_house_value']

test_data

"""# **Scoring(accuracy) for Linear Model**"""

reg.score(x_test, y_test)
#Acc-0.67

"""# **Random Forest**"""

from sklearn.ensemble import RandomForestRegressor
forest=RandomForestRegressor()
forest.fit(x_train,y_train)

forest.score(x_test, y_test)
#0.81

from sklearn.model_selection import GridSearchCV
forest=RandomForestRegressor()
param_grid={
    "n_estimators":[100,200,300],
    "max_features":[2,4],
    "max_depth": [None,4,8]
}
grid_search=GridSearchCV(forest, param_grid, cv=5, scoring="neg_mean_squarred_error", return_train_score=True)
grid_search.fit(x_train, y_train)

best_forest=grid_search.best_estimator_#n_estimators=200
best_forest.score(x_test, y_test)
#acc-81.5%